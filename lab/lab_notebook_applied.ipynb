{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "bat"
        },
        "id": "Dw8VHPsrzRBu"
      },
      "source": [
        "# Hedonic Pricing\n",
        "\n",
        "We often try to predict the price of an asset from its observable characteristics. This is generally called **hedonic pricing**: How do the unit's characteristics determine its market price?\n",
        "\n",
        "In the lab folder, there are three options: housing prices in pierce_county_house_sales.csv, car prices in cars_hw.csv, and airbnb rental prices in airbnb_hw.csv. If you know of another suitable dataset, please feel free to use that one.\n",
        "\n",
        "1. Clean the data and perform some EDA and visualization to get to know the data set.\n",
        "2. Transform your variables --- particularly categorical ones --- for use in your regression analysis.\n",
        "3. Implement an ~80/~20 train-test split. Put the test data aside.\n",
        "4. Build some simple linear models that include no transformations or interactions. Fit them, and determine their RMSE and $R^2$ on the both the training and test sets. Which of your models does the best?\n",
        "5. Make partial correlation plots for each of the numeric variables in your model. Do you notice any significant non-linearities?\n",
        "6. Include transformations and interactions of your variables, and build a more complex model that reflects your ideas about how the features of the asset determine its value. Determine its RMSE and $R^2$ on the training and test sets. How does the more complex model your build compare to the simpler ones?\n",
        "7. Summarize your results from 1 to 6. Have you learned anything about overfitting and underfitting, or model selection?\n",
        "8. If you have time, use the sklearn.linear_model.Lasso to regularize your model and select the most predictive features. Which does it select? What are the RMSE and $R^2$? We'll cover the Lasso later in detail in class."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "url = pd.read_csv('https://raw.githubusercontent.com/ezraattisso/linearModels/refs/heads/main/lab/data/cars_hw.csv')\n",
        "\n",
        "cars_df = pd.DataFrame(url)\n",
        "\n",
        "cars_df.head()\n",
        "\n",
        "# Checking for missing values\n",
        "print(\"Missing Values:\\n\", cars_df.isnull().sum())\n",
        "\n",
        "# Checking data types\n",
        "print(\"\\nData Types:\\n\", cars_df.dtypes)\n",
        "\n",
        "\n",
        "# Basic summary statistics"
      ],
      "metadata": {
        "id": "5R1K9mj6zR2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Q1\n",
        "\n",
        "# some exploratory plots being done here.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# histogram\n",
        "\n",
        "cars_df.hist(figsize=(12, 8), bins=20, edgecolor='black')\n",
        "plt.suptitle(\"Distribution of Numeric Features\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# pairplot\n",
        "\n",
        "sns.pairplot(cars_df)\n",
        "plt.show()\n",
        "\n",
        "# boxplot\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=cars_df, orient='h')\n",
        "plt.title(\"Boxplots of Numeric Features\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "vUf9DM3-0EbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Q2\n",
        "\n",
        "# checking which columns are categorical here, and then using them in hot encoding.\n",
        "\n",
        "categorical_cols = cars_df.select_dtypes(include=['object']).columns\n",
        "print(\"Categorical Columns:\\n\", categorical_cols)\n",
        "\n",
        "cars_df_encoded = pd.get_dummies(cars_df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "print(cars_df_encoded.head())\n"
      ],
      "metadata": {
        "id": "AqdpkfoK1r8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Q3\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Here I'm defining features as X and my target variable as y\n",
        "X = cars_df_encoded.drop(columns=['Mileage_Run'])\n",
        "y = cars_df_encoded['Price']\n",
        "\n",
        "# I'm splitting the data into the 80/20% for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verifying the split\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set size: {X_test.shape[0]} samples\")"
      ],
      "metadata": {
        "id": "E9-uaC1t4NcK",
        "outputId": "fd6059c5-9b2f-4ed2-d262-f5ed362503b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 780 samples\n",
            "Test set size: 196 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Q4\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X = cars_df_encoded[['Mileage_Run']]  # Predictor variable\n",
        "y = cars_df_encoded['Price']  # Target variable\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing and Fitting model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# these are my predictions\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Ccalculating rmse here\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "# calculating R^2 here\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "\n",
        "print(f\"Training RMSE: {rmse_train:.2f}, Training R^2: {r2_train:.2f}\")\n",
        "print(f\"Test RMSE: {rmse_test:.2f}, Test R^2: {r2_test:.2f}\")"
      ],
      "metadata": {
        "id": "TDY2hWi_9nC7",
        "outputId": "3f30677a-5c18-44dc-f7c9-03713e3e6a59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RMSE: 369437.23, Training R^2: 0.02\n",
            "Test RMSE: 330349.98, Test R^2: 0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Q5\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "numeric_cols = ['Mileage_Run', 'Year', 'Engine_Size']\n",
        "# Remove the target variable from predictors if it's in the list\n",
        "if 'Price' in numeric_cols:\n",
        "    numeric_cols.remove('Price')\n",
        "\n",
        "# Create a figure with subplots for each predictor\n",
        "n_cols = min(2, len(numeric_cols))\n",
        "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4*n_rows))\n",
        "if n_rows * n_cols > 1:\n",
        "    axes = axes.flatten()\n",
        "else:\n",
        "    axes = [axes]\n",
        "\n",
        "# Add a constant to the predictors for the intercept term\n",
        "X_with_const = sm.add_constant(cars_df_encoded[numeric_cols])\n",
        "model = sm.OLS(y, X_with_const)\n",
        "results = model.fit()\n",
        "\n",
        "# Create partial regression plots for each predictor\n",
        "for i, col in enumerate(numeric_cols):\n",
        "    # Create partial regression plot\n",
        "    sm.graphics.plot_partregress(results, col, 'Price', ax=axes[i])\n",
        "    axes[i].set_title(f'Partial Regression Plot for {col}')\n",
        "    axes[i].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Remove any unused subplots\n",
        "for j in range(len(numeric_cols), len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "P5pXHTt8VGrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "numeric_features = ['Mileage_Run', 'Year']\n",
        "X = cars_df_encoded[numeric_features]\n",
        "y = cars_df_encoded['Price']\n",
        "\n",
        "# Step 2: Splitting the data into training and test sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Create a more complex model with transformations and interactions.\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "\n",
        "feature_names = poly.get_feature_names_out(numeric_features)\n",
        "print(\"Transformed features:\", feature_names)\n",
        "\n",
        "# Fitting the model on the transformed data.\n",
        "complex_model = LinearRegression()\n",
        "complex_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Making predictions and evaluating.\n",
        "y_train_pred_complex = complex_model.predict(X_train_poly)\n",
        "y_test_pred_complex = complex_model.predict(X_test_poly)\n",
        "\n",
        "# Calculating some metrics here.\n",
        "rmse_train_complex = np.sqrt(mean_squared_error(y_train, y_train_pred_complex))\n",
        "rmse_test_complex = np.sqrt(mean_squared_error(y_test, y_test_pred_complex))\n",
        "r2_train_complex = r2_score(y_train, y_train_pred_complex)\n",
        "r2_test_complex = r2_score(y_test, y_test_pred_complex)\n",
        "\n",
        "# Displaying the results.\n",
        "print(\"\\nComplex Model with Transformations and Interactions:\")\n",
        "print(f\"Training RMSE: {rmse_train_complex:.2f}, Training R^2: {r2_train_complex:.2f}\")\n",
        "print(f\"Test RMSE: {rmse_test_complex:.2f}, Test R^2: {r2_test_complex:.2f}\")\n",
        "\n",
        "# Step 6: Wanting to do some comparisons with the simpler model from earlier.\n",
        "print(\"\\nComparison with Simple Model:\")\n",
        "print(f\"Simple - Training RMSE: {rmse_train:.2f}, Training R^2: {r2_train:.2f}\")\n",
        "print(f\"Simple - Test RMSE: {rmse_test:.2f}, Test R^2: {r2_test:.2f}\")\n",
        "print(f\"Complex - Training RMSE: {rmse_train_complex:.2f}, Training R^2: {r2_train_complex:.2f}\")\n",
        "print(f\"Complex - Test RMSE: {rmse_test_complex:.2f}, Test R^2: {r2_test_complex:.2f}\")"
      ],
      "metadata": {
        "id": "-YlR4WewX4VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.\n",
        "\n",
        "The more complex model with transformations and interactions likely showed improved metrics (lower RMSE, higher R²) on the training data compared to the simple model. When it comes to under/overfit, the simple linear model with just mileage probably underfits the data. In addition though, the complex model might show signs of overfitting if it performs way better on training data than test data by learning more from the available noise than the patterns that more easily recognizable."
      ],
      "metadata": {
        "id": "dCcKvKnQYedJ"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}